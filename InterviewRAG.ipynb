{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP1Jf3fZP5EZoY163ZOsBPO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lannd3217/Advanced-Business-Analysis/blob/main/InterviewRAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H3bU7oZbrCLx"
      },
      "outputs": [],
      "source": [
        "# !git clone https://github.com/lannd3217/Interview_RAG.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "GIT_TOKEN = userdata.get('GITHUB_TOKEN')\n",
        "GIT_USER = \"lannd3217\"\n",
        "GIT_REPO = \"Interview_RAG\"\n",
        "GIT_EMAIL = \"lanngocd.17@gmail.com\"\n",
        "GIT_NAME = \"LAN DINH\"\n",
        "\n",
        "!git config --global user.email {GIT_EMAIL}\n",
        "!git config --global user.name {GIT_NAME}\n",
        "remote_url = f\"https://{GIT_TOKEN}@github.com/{GIT_USER}/{GIT_REPO}.git\"\n",
        "!git clone {remote_url}"
      ],
      "metadata": {
        "id": "V4DknEiY2-8J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%ls"
      ],
      "metadata": {
        "id": "-Dtz77S05U9q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd \"Interview_RAG\""
      ],
      "metadata": {
        "id": "0R3xATlybwQJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install -U langchain langchain-community langchain-openai\n",
        "!pip install -U langchain langchain-community langchain-text-splitters\n",
        "!pip install -U pymupdf langchain-community\n",
        "!pip install -U langchain-huggingface sentence-transformers\n",
        "!pip install -q sentence-transformers faiss-cpu transformers\n",
        "!pip install langchain langchain-community langchain-chroma langchain-huggingface pymupdf sentence-transformers\n",
        "!pip install \"unstructured[all-docs]\"\n"
      ],
      "metadata": {
        "id": "hgelsbhVdEFQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "# Updated import path\n",
        "from langchain_core.documents import Document\n",
        "from langchain_community.document_loaders import PyMuPDFLoader, TextLoader, CSVLoader, DirectoryLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter"
      ],
      "metadata": {
        "id": "f-3rFBaW-8_l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## LOADING\n",
        "from langchain_community.document_loaders import DirectoryLoader, PyMuPDFLoader, TextLoader\n",
        "\n",
        "def load_docs_from_folder(folder_path=\"./docs\"):\n",
        "    \"\"\"\n",
        "    Loads all PDF and TXT documents from a specified directory.\n",
        "    \"\"\"\n",
        "    print(f\"Loading documents from: {folder_path}...\")\n",
        "\n",
        "    # Define the mapping of file extensions to loader classes\n",
        "    loaders_mapping = {\n",
        "        \"*.pdf\": PyMuPDFLoader,\n",
        "        \"*.txt\": TextLoader\n",
        "    }\n",
        "\n",
        "    docs = []\n",
        "\n",
        "    # We loop through the mapping to ensure each file type is handled correctly\n",
        "    for glob_pattern, loader_class in loaders_mapping.items():\n",
        "        loader = DirectoryLoader(\n",
        "            folder_path,\n",
        "            glob=glob_pattern,\n",
        "            loader_cls=loader_class,\n",
        "            show_progress=True,\n",
        "            silent_errors=True\n",
        "        )\n",
        "        docs.extend(loader.load())\n",
        "\n",
        "    print(f\"Successfully loaded {len(docs)} document objects.\")\n",
        "    return docs"
      ],
      "metadata": {
        "id": "qs5n3IFX5yf5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs = load_docs_from_folder(\"./docs\")"
      ],
      "metadata": {
        "id": "jEJDjSfu50ZU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "search_term = \"A/B testing\"\n",
        "for doc in docs:\n",
        "    if search_term.lower() in doc.page_content.lower():\n",
        "        print(f\"Found '{search_term}' in {doc.metadata['source']} on page {doc.metadata['page']}\")"
      ],
      "metadata": {
        "id": "sfHX1NeeeX8e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "## CLEANING\n",
        "\n",
        "import re\n",
        "\n",
        "def clean_text(text, is_reddit=False):\n",
        "    \"\"\"\n",
        "    Unified cleaning function for both PDF and Reddit/HTML content.\n",
        "    \"\"\"\n",
        "    # --- PART 1: Reddit/HTML Specific Cleaning ---\n",
        "    if is_reddit:\n",
        "        # Remove Top Navigation and Sidebar noise\n",
        "        text = re.sub(r\"Skip to main content.*?Go to datascience\", \"\", text, flags=re.DOTALL)\n",
        "\n",
        "        # Remove UI buttons and repetitive Reddit metadata\n",
        "        ui_patterns = [\n",
        "            r\"Upvote\", r\"Downvote\", r\"Award\", r\"Share\", r\"Open chat\",\n",
        "            r\"Create post\", r\"Open inbox\", r\"Expand user menu\",\n",
        "            r\"More replies\", r\"1 more reply\", r\"Archived post.*?\\.\",\n",
        "            r\"Sort by:\", r\"Best\", r\"Search Comments\", r\"Comments Section\",\n",
        "            r\"avatar\", r\"â€¢\", r\"\\d+mo ago\", r\"\\d+h ago\"\n",
        "        ]\n",
        "        for pattern in ui_patterns:\n",
        "            text = re.sub(pattern, \"\", text, flags=re.IGNORECASE)\n",
        "\n",
        "        # Normalize User Handles (e.g., u/User avatar User -> User: User)\n",
        "        text = re.sub(r\"u/(\\w+)\\s+avatar\\s+\\1\", r\"\\nUser: \\1\\n\", text)\n",
        "\n",
        "    # --- PART 2: General/PDF Specific Cleaning ---\n",
        "    # 1. Remove weird \"cid\" characters\n",
        "    text = re.sub(r'\\(cid:\\d+\\)', '', text)\n",
        "\n",
        "    # 2. Remove purely numerical lines (page numbers/footers)\n",
        "    text = re.sub(r'^\\d+$\\n', '', text, flags=re.MULTILINE)\n",
        "\n",
        "    # --- PART 3: Whitespace Normalization (Apply to all) ---\n",
        "    # Replace multiple newlines with a single one\n",
        "    text = re.sub(r'\\n\\s*\\n', '\\n', text)\n",
        "    # Replace multiple spaces with a single space\n",
        "    text = re.sub(r' +', ' ', text)\n",
        "\n",
        "    return text.strip()\n",
        "\n",
        "# --- Apply cleaning and create the hashmap ---\n",
        "cleaned_samples = {}\n",
        "\n",
        "for doc in docs:\n",
        "    # 1. Extract raw content\n",
        "    raw_content = doc.page_content\n",
        "\n",
        "    # 2. Get the filename from metadata\n",
        "    filename = doc.metadata.get('source', 'Unknown File')\n",
        "\n",
        "    # 3. Detect if it's a Reddit/Forum file\n",
        "    is_reddit_file = \"reddit\" in filename.lower() or \".txt\" in filename.lower()\n",
        "\n",
        "    # 4. Clean the text\n",
        "    cleaned_content = clean_text(raw_content, is_reddit=is_reddit_file)\n",
        "\n",
        "    # 5. Store in hashmap (merging pages if the file has multiple)\n",
        "    if filename in cleaned_samples:\n",
        "        cleaned_samples[filename] += \"\\n\" + cleaned_content\n",
        "    else:\n",
        "        cleaned_samples[filename] = cleaned_content\n",
        "\n",
        "# --- Verify the results ---\n",
        "# print(f\"Success! Cleaned and indexed {len(cleaned_samples)} unique documents.\")\n",
        "# cleaned_samples[\"docs/Reddit_DS_NewGrad.txt\"]\n",
        "# --- Print the Example Output ---\n",
        "print(f\"Processed {len(cleaned_samples)} unique documents.\\n\")\n",
        "\n",
        "# We convert the dictionary items to a list so we can slice the first 3\n",
        "for filename, text in list(cleaned_samples.items())[:3]:\n",
        "    print(f\"Source: {filename}\")\n",
        "    print(f\"--- Cleaned Text Preview ---\\n{text[:350]}...\")\n",
        "    print(\"-\" * 30)"
      ],
      "metadata": {
        "id": "ZTXlTAjJ6MwO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## SPLITTING\n",
        "\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Initialize the splitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,   # Roughly 200 words\n",
        "    chunk_overlap=200, # Overlap helps keep context between chunks\n",
        "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
        ")\n",
        "\n",
        "# Create the chunks\n",
        "chunks = text_splitter.split_documents(docs)\n",
        "print(f\"Created {len(chunks)} chunks from {len(cleaned_samples)} documents.\")"
      ],
      "metadata": {
        "id": "rJMaWfJEgGwH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "from langchain_core.documents import Document\n",
        "from langchain_community.document_loaders import PyMuPDFLoader, TextLoader, CSVLoader, DirectoryLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "def clean_text(text, is_reddit=False):\n",
        "    \"\"\"Refined unified cleaning logic.\"\"\"\n",
        "    if is_reddit:\n",
        "        # Remove Reddit-specific UI noise\n",
        "        text = re.sub(r\"Skip to main content.*?Go to datascience\", \"\", text, flags=re.DOTALL)\n",
        "        ui_patterns = [r\"Upvote\", r\"Downvote\", r\"Award\", r\"Share\", r\"avatar\", r\"â€¢\", r\"\\d+mo ago\"]\n",
        "        for p in ui_patterns:\n",
        "            text = re.sub(p, \"\", text, flags=re.IGNORECASE)\n",
        "\n",
        "    text = re.sub(r'\\(cid:\\d+\\)', '', text) # PDF artifacts\n",
        "    text = re.sub(r'^\\d+$\\n', '', text, flags=re.MULTILINE) # Page numbers\n",
        "    text = re.sub(r'\\n\\s*\\n', '\\n', text) # Normalize newlines\n",
        "    text = re.sub(r' +', ' ', text) # Normalize spaces\n",
        "    return text.strip()\n",
        "\n",
        "def ingestion_pipeline(folder_path=\"./docs\"):\n",
        "    # 1. LOAD\n",
        "    print(f\"Starting ingestion from: {folder_path}\")\n",
        "\n",
        "    # Map extensions to specific loaders\n",
        "    loaders_mapping = {\n",
        "        \"*.pdf\": PyMuPDFLoader,\n",
        "        \"*.txt\": TextLoader,\n",
        "        \"*.csv\": CSVLoader\n",
        "    }\n",
        "\n",
        "    raw_docs = []\n",
        "    for glob_pattern, loader_cls in loaders_mapping.items():\n",
        "        # Only attempt to load if the folder actually contains these files\n",
        "        loader = DirectoryLoader(\n",
        "            folder_path,\n",
        "            glob=glob_pattern,\n",
        "            loader_cls=loader_cls,\n",
        "            silent_errors=True\n",
        "        )\n",
        "        raw_docs.extend(loader.load())\n",
        "\n",
        "    if not raw_docs:\n",
        "        print(\"No documents found. Check your folder path or file extensions.\")\n",
        "        return []\n",
        "\n",
        "    # 2. CLEAN & 4. ATTACH METADATA\n",
        "    cleaned_docs_map = {}\n",
        "    for doc in raw_docs:\n",
        "        source = doc.metadata.get('source', 'Unknown')\n",
        "        # Identify Reddit content for specialized cleaning\n",
        "        is_reddit = \"reddit\" in source.lower() or source.endswith(\".txt\")\n",
        "\n",
        "        cleaned_content = clean_text(doc.page_content, is_reddit=is_reddit)\n",
        "\n",
        "        if source not in cleaned_docs_map:\n",
        "            cleaned_docs_map[source] = {\"text\": \"\", \"metadata\": doc.metadata}\n",
        "        cleaned_docs_map[source][\"text\"] += \"\\n\" + cleaned_content\n",
        "\n",
        "    # 3. SPLIT\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=600,\n",
        "        chunk_overlap=100,\n",
        "        length_function=len\n",
        "    )\n",
        "\n",
        "    final_document_objects = []\n",
        "    doc_chunk_counts = {}\n",
        "\n",
        "    for source, data in cleaned_docs_map.items():\n",
        "        # Create temporary document for splitting\n",
        "        temp_doc = Document(page_content=data[\"text\"], metadata=data[\"metadata\"])\n",
        "        chunks = text_splitter.split_documents([temp_doc])\n",
        "\n",
        "        # Enrich metadata\n",
        "        for chunk in chunks:\n",
        "            chunk.metadata[\"doc_name\"] = os.path.basename(source)\n",
        "            chunk.metadata[\"section\"] = \"Main Content\"\n",
        "            final_document_objects.append(chunk)\n",
        "\n",
        "        doc_chunk_counts[os.path.basename(source)] = len(chunks)\n",
        "\n",
        "    # 5. OUTPUT METRICS\n",
        "    total_chunks = len(final_document_objects)\n",
        "    avg_size = sum(len(d.page_content) for d in final_document_objects) / total_chunks if total_chunks > 0 else 0\n",
        "\n",
        "    print(\"\\n--- Ingestion Metrics ---\")\n",
        "    print(f\"Total Chunks Created:  {total_chunks}\")\n",
        "    print(f\"Average Chunk Size:    {avg_size:.2f} chars\")\n",
        "    print(f\"Metadata Coverage:     100% (doc_name, section attached)\")\n",
        "    print(\"\\n--- Chunks per Document ---\")\n",
        "    for name, count in doc_chunk_counts.items():\n",
        "        print(f\"- {name}: {count} chunks\")\n",
        "\n",
        "    return final_document_objects\n",
        "\n",
        "# Execute\n",
        "# Note: Ensure you have run !pip install langchain-core langchain-community pymupdf\n",
        "docs_for_vectorstore = ingestion_pipeline(\"./docs\")"
      ],
      "metadata": {
        "id": "W3KkPcMi-ttF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs_for_vectorstore[3].page_content"
      ],
      "metadata": {
        "id": "_HdjDXuX_pV5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ## EMBEDDING\n",
        "# from langchain_huggingface import HuggingFaceEmbeddings\n",
        "# from langchain_community.vectorstores import FAISS\n",
        "\n",
        "# # 1. Initialize the Embedding Model\n",
        "# print(\"Downloading embedding model (MiniLM-L6)...\")\n",
        "# embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# # 2. Create the Vector Store from your full document list\n",
        "# print(f\"Embedding {len(docs_for_vectorstore)} chunks into FAISS...\")\n",
        "# # This turns the text into high-dimensional vectors\n",
        "# vector_db = FAISS.from_documents(docs_for_vectorstore, embeddings)\n",
        "\n",
        "# # 3. Save the index locally\n",
        "# vector_db.save_local(\"faiss_interview_index\")\n",
        "# print(\"FAISS Index saved and ready for retrieval!\")"
      ],
      "metadata": {
        "id": "znjm14RQBJX6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from langchain_huggingface import HuggingFaceEmbeddings\n",
        "# from langchain_chroma import Chroma\n",
        "\n",
        "# print(\"\\nDownloading embedding model (MiniLM-L6)...\")\n",
        "# embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# # 3. Create ChromaDB (Local-First Prototyping)\n",
        "# print(f\"Embedding and storing in ChromaDB...\")\n",
        "# vector_db = Chroma.from_documents(\n",
        "#     documents=docs_for_vectorstore,\n",
        "#     embedding=embeddings,\n",
        "#     persist_directory=\"./interview_knowledge_base\",\n",
        "#     collection_name=\"interview_prep\"\n",
        "# )\n",
        "\n",
        "# print(f\"\\n SUCCESS: Vector Store created with {vector_db._collection.count()} records.\")"
      ],
      "metadata": {
        "id": "KTnO4Gz7MX0n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Fetch a sample from your collection to inspect components\n",
        "# sample_data = vector_db.get(limit=100, include=['embeddings', 'documents', 'metadatas'])\n",
        "\n",
        "# # --- 1. CHECK EMBEDDINGS (The 384-dim vectors) ---\n",
        "# embedding_sample = sample_data['embeddings'][50]\n",
        "# print(f\"Embeddings Found: Vector length is {len(embedding_sample)}\")\n",
        "# # Should be 384 if using all-MiniLM-L6-v2\n",
        "\n",
        "# # --- 2. CHECK SOURCE TEXT (Original chunk) ---\n",
        "# document_sample = sample_data['documents'][1]\n",
        "# print(f\"Source Text Found: '{document_sample[:50]}...'\")\n",
        "\n",
        "# # --- 3. CHECK METADATA (source_doc, page, section, etc.) ---\n",
        "# metadata_sample = sample_data['metadatas'][1]\n",
        "# print(f\"Metadata Found: {metadata_sample.keys()}\")\n",
        "# # Look for: 'doc_name', 'section', 'source'\n",
        "\n",
        "# # --- 4. CHECK IDs (Unique identifiers) ---\n",
        "# id_sample = sample_data['ids'][0]\n",
        "# print(f\"Unique ID Found: {id_sample}\")"
      ],
      "metadata": {
        "id": "FakQhifuQHWf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## EMBEDDING\n",
        "## VECTOR DATABASE\n",
        "import chromadb\n",
        "from chromadb.utils.embedding_functions import SentenceTransformerEmbeddingFunction\n",
        "import uuid\n",
        "\n",
        "client = chromadb.PersistentClient(path=\"./interview_vector_db\")\n",
        "\n",
        "embedding_func = SentenceTransformerEmbeddingFunction(model_name=\"all-MiniLM-L6-v2\")\n",
        "\n",
        "\n",
        "collection = client.get_or_create_collection(\n",
        "    name=\"interview_prep_collection\",\n",
        "    embedding_function=embedding_func\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "oJ8WlRJ7Qa4G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents_text = [doc.page_content for doc in docs_for_vectorstore]\n",
        "metadatas = [doc.metadata for doc in docs_for_vectorstore]\n",
        "ids = [str(uuid.uuid4()) for _ in range(len(docs_for_vectorstore))]\n",
        "# ids = [f\"id{i}\" for i in range(len(docs_for_vectorstore))]\n",
        "\n",
        "print(f\"Inserting {len(documents_text)} chunks into ChromaDB...\")\n",
        "collection.add(\n",
        "    ids=ids,\n",
        "    documents=documents_text,\n",
        "    metadatas=metadatas\n",
        ")\n",
        "\n",
        "print(f\"Collection created. Total items: {collection.count()}\")"
      ],
      "metadata": {
        "id": "JNdkhokqVpog"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List all collections\n",
        "print(client.list_collections())\n",
        "# collection.peek()\n",
        "collection.get(ids=['ab2c95cb-3f9b-4cec-92b1-8b7f06430faa'], include=[\"embeddings\"])"
      ],
      "metadata": {
        "id": "IV2GMMnWU0Zq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- Component Audit ---\")\n",
        "peek_result = collection.peek(1)\n",
        "print(f\"ID: {peek_result['ids'][0]}\")\n",
        "print(f\"Metadata: {peek_result['metadatas'][0]}\")\n",
        "print(f\"Document Preview: {peek_result['documents'][0][:50]}...\")\n",
        "\n",
        "# B. Semantic Search Test (Slide: \"Querying a Collection\")\n",
        "query = \"How do I handle a technical interview for a data science role?\"\n",
        "results = collection.query(\n",
        "    query_texts=[query],\n",
        "    n_results=3\n",
        ")\n",
        "\n",
        "print(f\"\\nSemantic Search Results for: '{query}'\")\n",
        "for i in range(len(results['documents'][0])):\n",
        "    print(f\"Result {i+1} (Distance: {results['distances'][0][i]:.4f}):\")\n",
        "    print(f\"Content: {results['documents'][0][i]}...\")\n",
        "    print(\"-\" * 20)"
      ],
      "metadata": {
        "id": "qffRklI1V5T5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results"
      ],
      "metadata": {
        "id": "AYX5lATMBK0z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from langchain_huggingface import HuggingFaceEmbeddings\n",
        "# from langchain_chroma import Chroma\n",
        "# ## RETRIEVAL\n",
        "# embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# # Instantiate Retriever\n",
        "# vector_store = Chroma.from_documents(\n",
        "#     documents = chunks,\n",
        "#     embedding = embeddings\n",
        "# )\n"
      ],
      "metadata": {
        "id": "gHdmhizlDqKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# retriever = vector_store.as_retriever(\n",
        "#     search_type = \"similarity\",\n",
        "#     search_kwargs = {\"k\":2}\n",
        "# )"
      ],
      "metadata": {
        "id": "R79PirPXFvJH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Create Prompt Template\n",
        "# from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "# prompt = ChatPrompTemplate.from_template(\"\"\"\n",
        "# Use the following pievces of context to answer the question at the end.\n",
        "# If you don't know the answer, say that you don't know.\n",
        "# Context: {context}\n",
        "# Question: {question}\n",
        "# \"\"\")"
      ],
      "metadata": {
        "id": "oxKmEUS8F9wi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from langchain_openai import ChatOpenAI\n",
        "# llm = ChatOpenAI(model =\"gpt-4o-mini\",api_key=userdata.get('OPEN_AI'), temperature=0)"
      ],
      "metadata": {
        "id": "1Za3o4_BGdf-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Assemble the Chain\n",
        "# from langchain_core.runnables import RunnablePassthrough\n",
        "# from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# chain = (\n",
        "#     {\"context\": retriever, \"question\": RunnablePassthrough}\n",
        "#     | prompt\n",
        "#     | llm\n",
        "#     | StrOutputParser\n",
        "# )"
      ],
      "metadata": {
        "id": "FOokUHnTIioP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from transformers import pipeline\n",
        "# import torch\n",
        "\n",
        "# # 1. RETRIEVE from your ChromaDB (as we did before)\n",
        "# user_query = \"What should I do if I don't know the answer to a technical question?\"\n",
        "# results = collection.query(\n",
        "#     query_texts=[user_query],\n",
        "#     n_results=2 # Keep context short for local models\n",
        "# )\n",
        "# retrieved_context = \" \".join(results['documents'][0])\n",
        "\n",
        "# # 2. INITIALIZE LOCAL LLM (No API Key Required)\n",
        "# # We use flan-t5-large because it's optimized for instructions\n",
        "# print(\"â³ Loading Flan-T5 model (approx 1.5GB)...\")\n",
        "# model_id = \"google/flan-t5-large\"\n",
        "# generator = pipeline(\n",
        "#     \"text2text-generation\",\n",
        "#     model=model_id,\n",
        "#     device=0 if torch.cuda.is_available() else -1  # Uses GPU if available\n",
        "# )\n",
        "\n",
        "# # 3. CONSTRUCT THE PROMPT (Matches your \"Prompt Template\" slide)\n",
        "# # For T5, we use a specific instruction format\n",
        "# input_text = f\"answer the question using the context: {retrieved_context} Question: {user_query}\"\n",
        "\n",
        "# # 4. GENERATE\n",
        "# output = generator(input_text, max_length=150, do_sample=False)\n",
        "\n",
        "# print(\"\\nðŸ¤– LOCAL LLM RESPONSE (Flan-T5):\")\n",
        "# print(output[0]['generated_text'])"
      ],
      "metadata": {
        "id": "Bqbz-iF6JT6-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "# Instantiate Retriever\n",
        "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "vector_store = Chroma(\n",
        "    persist_directory=\"./interview_vector_db\",\n",
        "    embedding_function=embedding_model,\n",
        "    collection_name = \"interview_prep_collection\"\n",
        ")\n",
        "\n",
        "retriever = vector_store.as_retriever(\n",
        "    search_type=\"similarity\",\n",
        "    search_kwargs={\"k\": 2} # Fetches top-2 most relevant chunks\n",
        ")"
      ],
      "metadata": {
        "id": "S3wpblNxKbDa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_docs = retriever.invoke(\"technical interview\")\n",
        "print(f\"Success! Found {len(test_docs)} relevant chunks.\")\n",
        "if len(test_docs) > 0:\n",
        "    print(f\"Snippet: {test_docs[0].page_content[:100]}...\")"
      ],
      "metadata": {
        "id": "rJkBGtTJM7Ug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "# Create Prompt Template\n",
        "template = \"\"\"\n",
        "Use the following pieces of context to answer the question at the end.\n",
        "If don't know the answer, say I don't know.\n",
        "Context: {context}\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(template) #"
      ],
      "metadata": {
        "id": "K7L66Gi_KhdX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "id": "cBknMmLRRafo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_huggingface import HuggingFacePipeline\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "import torch\n",
        "\n",
        "# Initialize Free Local LLM (HuggingFace Flan-T5)\n",
        "llm = HuggingFacePipeline.from_model_id(\n",
        "    model_id=\"google/flan-t5-large\",\n",
        "    task=\"text2text-generation\",\n",
        "    device = 0 if torch.cuda.is_available() else -1,\n",
        "    pipeline_kwargs={\n",
        "        \"max_new_tokens\": 200,\n",
        "        \"truncation\": True,\n",
        "        \"max_length\": 512\n",
        "        }\n",
        ")\n",
        "\n",
        "# Assemble the Chain\n",
        "# chain = (\n",
        "#     {\"context\": retriever, \"question\": RunnablePassthrough()} #\n",
        "#     | prompt\n",
        "#     | llm\n",
        "#     | StrOutputParser() #\n",
        "# )\n",
        "\n",
        "\n",
        "\n",
        "# A helper function to extract ONLY the text content from retrieved documents\n",
        "def combine_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "# Re-assemble the Chain\n",
        "chain = (\n",
        "    {\n",
        "        \"context\": retriever | RunnableLambda(combine_docs), # Extracts only text\n",
        "        \"question\": RunnablePassthrough()\n",
        "    }\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")"
      ],
      "metadata": {
        "id": "R1mAMbcAKp7u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q = \"What is the best way to handle a technical interview?\"\n",
        "a = chain.invoke(q)\n",
        "print(a)"
      ],
      "metadata": {
        "id": "_zm7bBvUQx5B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_questions = [\n",
        "    \"What is the best way to handle a technical interview?\",\n",
        "    \"What do people on Reddit recommend doing if you get stuck on a coding question during a live interview?\",\n",
        "    \"What are some common 'red flags' to look for in a company's data science culture according to interviewees?\",\n",
        "    \"Based on the provided documents, how is the 'Bias-Variance Tradeoff' explained?\",\n",
        "    \"What are the primary differences between supervised and unsupervised learning mentioned in the text?\",\n",
        "    \"What are the three most common evaluation metrics for classification models according to the source?\",\n",
        "    \"How do I explain my data science projects?\",\n",
        "    \"What are the main mistakes candidates make in interviews?\"\n",
        "]\n",
        "\n",
        "print(\"Running Batch Test...\\n\")\n",
        "\n",
        "for q in test_questions:\n",
        "    print(f\"Question: {q}\")\n",
        "    answer = chain.invoke(q)\n",
        "    print(f\"Answer: {answer}\")\n",
        "    print(\"-\" * 40)"
      ],
      "metadata": {
        "id": "YklCIgzqKuZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "def deep_clean_notebook(filename):\n",
        "    with open(filename, 'r', encoding='utf-8') as f:\n",
        "        nb = json.load(f)\n",
        "\n",
        "    # Remove the problematic widgets metadata entirely\n",
        "    if 'metadata' in nb and 'widgets' in nb['metadata']:\n",
        "        print(\"Found and removing metadata.widgets...\")\n",
        "        del nb['metadata']['widgets']\n",
        "\n",
        "    # Also remove 'state' if it's hiding in the top level (rare but happens)\n",
        "    if 'state' in nb.get('metadata', {}):\n",
        "        del nb['metadata']['state']\n",
        "\n",
        "    new_filename = filename.replace('.ipynb', '_fixed.ipynb')\n",
        "    with open(new_filename, 'w', encoding='utf-8') as f:\n",
        "        json.dump(nb, f, indent=2)\n",
        "\n",
        "    print(f\"Success! Please try opening: {new_filename}\")\n",
        "\n",
        "# Change this to your actual filename\n",
        "deep_clean_notebook('InterviewRAG.ipynb')"
      ],
      "metadata": {
        "id": "vUw1s85ycGxd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git add --all"
      ],
      "metadata": {
        "id": "pLI1UlFVbUN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git commit -a -m \"Fixed notebook\""
      ],
      "metadata": {
        "id": "TuSRchuUbXD1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git push origin main --force"
      ],
      "metadata": {
        "id": "TWTAW4S8bj4i"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}